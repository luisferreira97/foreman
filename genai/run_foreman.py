import os
import json
import logging

from pathlib import Path
from dotenv import load_dotenv, find_dotenv

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts.chat import (
    ChatPromptTemplate,
)
from langchain_openai import ChatOpenAI

def load_json(path: str) -> dict:
    """
    Load a JSON file and return it as a dictionary.
    
    Args:
        path (str): The path to the JSON file.
        
    Returns:
        dict: The JSON file as a dictionary.
    """
    
    doc = json.loads(Path(path).read_text())

    return doc

def load_markdwon(path: str) -> str:
    """
    Load a Markdown file and return it as a string.
    
    Args:
        path (str): The path to the JSON file.
        
    Returns:
        dict: The JSON file as a dictionary.
    """
    
    with open(path, 'r') as file:
        markdown = file.read()

    return markdown


def get_context(docs: dict) -> str:
    """
    Get the context for the tables.
    Context includes the schema name, table name, table descriptions,
    columns, and their descriptions.

    Args:
        tables (list): List of tables.

    Returns:
        str: Context for the tables in string format.
    """

    context = []

    for table in docs:
        table_columns = [f"{column} ({docs[table]['columns'][column]['description']}) {docs[table]['columns'][column].get('type','')}" for column in docs[table]["columns"]]
        docstring = f"Table: {table}, description: {docs[table]['description']}, columns: [{'; '.join(table_columns)}]"
        context.append(docstring)

    context = list(set(context))
    context = "\n\n".join(context)
    logging.info(f"Context for the request: {context}")

    return context


def main():
    _ = load_dotenv(find_dotenv()) # read local .env file
    api_key = os.environ['OPENAI_API_KEY']

    llm = ChatOpenAI(model="gpt-4o", temperature=0)

    # Queries without RAG
    messages = [("system", "You are a friendly AI assistant, helping humans with their questions. Answer in one sentence."),]

    # Query 1 
    question = "What is Data Makers Fest?"
    print(f"Question: {question}\n")
    messages.append(("human", question))
    ai_msg = llm.invoke(messages)
    print(f"Answer: {ai_msg.content}\n")
    messages.append(("assistant", ai_msg.content))

    # Query 2
    question = "Where is it hosted?"
    print(f"Question: {question}\n")
    messages.append(("human", question))
    ai_msg = llm.invoke(messages)
    print(f"Answer: {ai_msg.content}\n")
    messages.append(("assistant", ai_msg.content))

    # Query 3
    question = "Tell me one of the cities where it has been hosted"
    print(f"Question: {question}\n")
    messages.append(("human", question))
    ai_msg = llm.invoke(messages)
    print(f"Answer: {ai_msg.content}\n")

    # Query 4
    question = "What types of sessions does it have?"
    print(f"Question: {question}\n")
    messages.append(("human", question))
    ai_msg = llm.invoke(messages)
    print(f"Answer: {ai_msg.content}\n")

    
    # Queries with RAG
    docs = load_json('docs.json')
    context = get_context(docs)
    overview = load_markdwon('../warehouse/models/overview.md')

    general_prompt = """
    You are a friendly AI assistant, helping humans with their questions regarding Data Makers Fest.
    Answer only the last question asked by the human.
    Use the following information to answer the question:
    
    Take into account the following overview information about the event:
    <event>
    {event}
    <event>
    
    Also take into account the following documentation generated by the data warehouse:
    <docs>
    {context}
    <docs>
    """

    prompt_template = ChatPromptTemplate.from_messages([("system", general_prompt)])

    # Query 1
    question = "What is Data Makers Fest?"
    print(f"Question: {question}\n")
    prompt_template.messages.append(HumanMessage(content=question))
    prompt = prompt_template.format(event=overview, context=context)
    ai_msg = llm.invoke(prompt)
    print(f"Answer: {ai_msg.content}\n")

    # Query 2
    question = "What types of sessions does it have?"
    print(f"Question: {question}\n")
    prompt_template.messages.append(HumanMessage(content=question))
    prompt = prompt_template.format(event=overview, context=context)
    ai_msg = llm.invoke(prompt)
    print(f"Answer: {ai_msg.content}\n")

    # Query 3
    question = "Where are the tutorials taking place?"
    print(f"Question: {question}\n")
    prompt_template.messages.append(HumanMessage(content=question))
    prompt = prompt_template.format(event=overview, context=context)
    ai_msg = llm.invoke(prompt)
    print(f"Answer: {ai_msg.content}\n")
    
    sql_prompt = """
    You are a friendly AI assistant, helping humans with their questions regarding Data Makers Fest.
    Answer only the last question asked by the human.
    The answer should be in SQL format.
    Just provide the SQL query, not the result.
    Use the following information to answer the question:
    
    Take into account the following overview information about the event:
    <event>
    {event}
    <event>
    
    Also take into account the following documentation generated by the data warehouse:
    <docs>
    {context}
    <docs>
    """

    prompt_template = ChatPromptTemplate.from_messages([("system", sql_prompt)])

    # Query 1
    question = "How many sessions are there in total?"
    print(f"\nQuestion: {question}")
    prompt_template.messages.append(HumanMessage(content=question))
    prompt = prompt_template.format(event=overview, context=context)
    ai_msg = llm.invoke(prompt)
    print("\nAnswer:")
    print(ai_msg.content.replace("```sql\n","").replace("\n```",""))

    # Query 2
    question = "How many partners of each type are there?"
    print(f"\nQuestion: {question}")
    prompt_template.messages.append(HumanMessage(content=question))
    prompt = prompt_template.format(event=overview, context=context)
    ai_msg = llm.invoke(prompt)
    print("\nAnswer:")
    print(ai_msg.content.replace("```sql\n","").replace("\n```",""))

    # Query 3
    question = "Are there any speakers with more than one session (including tutorials)?"
    print(f"\nQuestion: {question}")
    prompt_template.messages.append(HumanMessage(content=question))
    prompt = prompt_template.format(event=overview, context=context)
    ai_msg = llm.invoke(prompt)
    print("\nAnswer:")
    print(ai_msg.content.replace("```sql\n","").replace("\n```",""))

if __name__ == "__main__":
    main()

    