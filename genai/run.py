from run_foreman import *

import streamlit as st

_ = load_dotenv(find_dotenv()) # read local .env file
api_key = os.environ['OPENAI_API_KEY']

llm = ChatOpenAI(model="gpt-4o", temperature=0)

# Queries without RAG
messages = [("system", "You are a friendly AI assistant, helping humans with their questions. Answer in one sentence."),]

# Query 1 
base_question_1 = "What is Data Makers Fest?"
print(f"Question: {base_question_1}\n")
messages.append(("human", base_question_1))
base_ai_msg_1 = llm.invoke(messages)
print(f"Answer: {base_ai_msg_1.content}\n")
messages.append(("assistant", base_ai_msg_1.content))

# Query 2
base_question_2 = "Where is it hosted?"
print(f"Question: {base_question_2}\n")
messages.append(("human", base_question_2))
base_ai_msg_2 = llm.invoke(messages)
print(f"Answer: {base_ai_msg_2.content}\n")
messages.append(("assistant", base_ai_msg_2.content))

# Query 3
base_question_3 = "Tell me one of the cities where it has been hosted"
print(f"Question: {base_question_3}\n")
messages.append(("human", base_question_3))
base_ai_msg_3 = llm.invoke(messages)
print(f"Answer: {base_ai_msg_3.content}\n")

# Query 4
base_question_4 = "What types of sessions does it have?"
print(f"Question: {base_question_4}\n")
messages.append(("human", base_question_4))
base_ai_msg_4 = llm.invoke(messages)
print(f"Answer: {base_ai_msg_4.content}\n")

with st.chat_message(name="human", avatar = "👨‍🦱"):
    st.info(base_question_1)
with st.chat_message(name="ai", avatar = "🤖"):
    st.info(base_ai_msg_1.content)
    
with st.chat_message(name="human", avatar = "👨‍🦱"):
    st.info(base_question_2)
with st.chat_message(name="ai", avatar = "🤖"):
    st.info(base_ai_msg_2.content)
    
with st.chat_message(name="human", avatar = "👨‍🦱"):
    st.info(base_question_3)
with st.chat_message(name="ai", avatar = "🤖"):
    st.info(base_ai_msg_3.content)
    
with st.chat_message(name="human", avatar = "👨‍🦱"):
    st.info(base_question_4)
with st.chat_message(name="ai", avatar = "🤖"):
    st.info(base_ai_msg_4.content)


# Queries with RAG
docs = load_json('docs.json')
context = get_context(docs)
overview = load_markdwon('../warehouse/models/overview.md')

general_prompt = """
You are a friendly AI assistant, helping humans with their questions regarding Data Makers Fest.
Answer only the last question asked by the human.
Use the following information to answer the question:

Take into account the following overview information about the event:
<event>
{event}
<event>

Also take into account the following documentation generated by the data warehouse:
<docs>
{context}
<docs>
"""

prompt_template = ChatPromptTemplate.from_messages([("system", general_prompt)])

# Query 1
foreman_question_1 = "What is Data Makers Fest?"
print(f"Question: {foreman_question_1}\n")
prompt_template.messages.append(HumanMessage(content=foreman_question_1))
prompt = prompt_template.format(event=overview, context=context)
foreman_ai_msg_1 = llm.invoke(prompt)
print(f"Answer: {foreman_ai_msg_1.content}\n")

# Query 2
foreman_question_2 = "What types of sessions does it have?"
print(f"Question: {foreman_question_2}\n")
prompt_template.messages.append(HumanMessage(content=foreman_question_2))
prompt = prompt_template.format(event=overview, context=context)
foreman_ai_msg_2 = llm.invoke(prompt)
print(f"Answer: {foreman_ai_msg_2.content}\n")

# Query 3
foreman_question_3 = "Where are the tutorials taking place?"
print(f"Question: {foreman_question_3}\n")
prompt_template.messages.append(HumanMessage(content=foreman_question_3))
prompt = prompt_template.format(event=overview, context=context)
foreman_ai_msg_3 = llm.invoke(prompt)
print(f"Answer: {foreman_ai_msg_3.content}\n")

with st.chat_message(name="human", avatar = "👨‍🦱"):
    st.info(foreman_question_1)
with st.chat_message(name="ai", avatar = "👷‍♂️"):
    st.info(foreman_ai_msg_1.content)
    
with st.chat_message(name="human", avatar = "👨‍🦱"):
    st.info(foreman_question_2)
with st.chat_message(name="ai", avatar = "👷‍♂️"):
    st.info(foreman_ai_msg_2.content)
    
with st.chat_message(name="human", avatar = "👨‍🦱"):
    st.info(foreman_question_3)
with st.chat_message(name="ai", avatar = "👷‍♂️"):
    st.info(foreman_ai_msg_3.content)
    
    

sql_prompt = """
You are a friendly AI assistant, helping humans with their questions regarding Data Makers Fest.
Answer only the last question asked by the human.
The answer should be in SQL format.
Just provide the SQL query, not the result.
Use the following information to answer the question:

Take into account the following overview information about the event:
<event>
{event}
<event>

Also take into account the following documentation generated by the data warehouse:
<docs>
{context}
<docs>
"""

prompt_template = ChatPromptTemplate.from_messages([("system", sql_prompt)])

# Query 1
sql_question_1 = "How many sessions are there in total?"
print(f"\nQuestion: {sql_question_1}")
prompt_template.messages.append(HumanMessage(content=sql_question_1))
prompt = prompt_template.format(event=overview, context=context)
sql_ai_msg_1 = llm.invoke(prompt)
print("\nAnswer:")
print(sql_ai_msg_1.content.replace("```sql\n","").replace("\n```",""))

# Query 2
sql_question_2 = "How many partners of each type are there?"
print(f"\nQuestion: {sql_question_2}")
prompt_template.messages.append(HumanMessage(content=sql_question_2))
prompt = prompt_template.format(event=overview, context=context)
sql_ai_msg_2 = llm.invoke(prompt)
print("\nAnswer:")
print(sql_ai_msg_2.content.replace("```sql\n","").replace("\n```",""))

# Query 3
sql_question_3 = "Are there any speakers with more than one participation?"
print(f"\nQuestion: {sql_question_3}")
prompt_template.messages.append(HumanMessage(content=sql_question_3))
prompt = prompt_template.format(event=overview, context=context)
sql_ai_msg_3 = llm.invoke(prompt)
print("\nAnswer:")
print(sql_ai_msg_3.content.replace("```sql\n","").replace("\n```",""))

# Query 4
sql_question_4 = "What is the number of organizations that have speakers in the event?"
print(f"\nQuestion: {sql_question_4}")
prompt_template.messages.append(HumanMessage(content=sql_question_4))
prompt = prompt_template.format(event=overview, context=context)
sql_ai_msg_4 = llm.invoke(prompt)
print("\nAnswer:")
print(sql_ai_msg_4.content.replace("```sql\n","").replace("\n```",""))


with st.chat_message(name="human", avatar = "👨‍🦱"):
    st.info(sql_question_1)
with st.chat_message(name="ai", avatar = "👷‍♂️"):
    st.info(sql_ai_msg_1.content)
with st.chat_message(name="human", avatar = "👨‍🦱"):
    st.info(sql_question_2)
with st.chat_message(name="ai", avatar = "👷‍♂️"):
    st.info(sql_ai_msg_2.content)
with st.chat_message(name="human", avatar = "👨‍🦱"):
    st.info(sql_question_3)
with st.chat_message(name="ai", avatar = "👷‍♂️"):
    st.info(sql_ai_msg_3.content)
with st.chat_message(name="human", avatar = "👨‍🦱"):
    st.info(sql_question_4)
with st.chat_message(name="ai", avatar = "👷‍♂️"):
    st.info(sql_ai_msg_4.content)